# Projet-Data-ENG

Suites d'outils et d'infrastructure pour collecter, d√©poser, pr√©parer puis publier des donn√©es territoriales (CSV locaux, API, scraping) sur Azure. Le projet automatise les √©tapes critiques : stockage dans un Data Lake, pr√©paration analytique, et chargement vers Azure SQL Database.

## 1. Vue d‚Äôensemble

Flux principal :

1. **Collecte des sources**  
   - CSV d√©pos√©s dans `uploads/landing/` (ex : statistiques INSEE, d√©mographie).  
   - API `ingestion/fetch_communes.py` pour r√©cup√©rer un JSON consolid√© des communes.  
   - Jeux scrapp√©s/externes ajout√©s dans le dossier `uploads/`.
2. **Atterrissage dans Azure Data Lake**  
   - `terraform apply` peut t√©l√©verser automatiquement `uploads/landing/` dans le filesystem `raw`.  
   - Le script `analytics/data_loader.py` permet aussi de rapatrier les donn√©es depuis ADLS.
3. **Pr√©paration & nettoyage**  
   - Notebook `analytics/notebooks/data_preparation.ipynb` ou module Python `analytics.lib.data_prep` pour transformer les fichiers en tables normalis√©es (`stg_population`, `dim_commune`, etc.).
4. **Publication dans Azure SQL Database**  
   - `analytics/export_to_sql.py` charge les tables pr√©par√©es vers la base `projet_data_eng` (sch√©ma `dbo` par d√©faut).  
   - Le script lit automatiquement la configuration SQL dans `Terraform/terraform.tfvars`.

![Flux logique](docs/img/dataflow.png "Collecte -> Data Lake -> Pr√©paration -> SQL") *(image optionnelle √† ajouter)*

## 2. Pr√©requis

| Outil | Version mini | Commentaires |
|-------|--------------|--------------|
| Terraform | 1.5+ | Provisionnement infra |
| Azure CLI | 2.52+ | Authentification & diagnostics |
| Python | 3.10+ | Scripts d‚Äôingestion et de pr√©paration |
| ODBC Driver for SQL Server | 18 (ou 17) | Requis pour l‚Äôexport vers Azure SQL |

Installer les d√©pendances Python :

```powershell
python -m pip install -r requirements.txt
```

## 3. Provisionner l‚Äôinfrastructure

```powershell
cd Terraform
terraform init
terraform plan
terraform apply
```

Variables essentielles √† d√©finir dans `Terraform/terraform.tfvars` :

```hcl
resource_group_name       = "rg-exemple"
resource_group_location   = "francecentral"
datalake_storage_account_name = "nomstockage"
key_vault_name            = "kv-exemple"
data_factory_name         = "adf-exemple"

# Azure SQL
sql_server_name        = "sqlexemple"
sql_admin_login        = "sqladmin"
sql_admin_password     = "MotDePasse!2024"
sql_database_name      = "projet_data_eng"
sql_firewall_rules = [
  { name = "Home", start_ip = "X.X.X.X", end_ip = "X.X.X.X" }
]
```

Sorties Terraform utiles :

```powershell
terraform output datalake_primary_connection_string
terraform output sql_server_fqdn
terraform output sql_database_name
```

## 4. Collecter et d√©poser les donn√©es

### 4.1 Chargement local -> Data Lake

1. Placer les fichiers CSV/XLSX dans `uploads/landing/` (structure libre).  
2. Activer l‚Äôupload automatique :
   ```hcl
   upload_files_enabled = true
   upload_source_dir    = "../uploads/landing"
   upload_datalake_filesystem = "raw"
   ```
3. `terraform apply` t√©l√©verse les fichiers dans `abfss://raw@<storage>.dfs.core.windows.net/`.

### 4.2 Ingestion API communes

```powershell
python ingestion/fetch_communes.py `
  --connection-string "<cha√Æne ADLS>" `
  --departements 02 59 60 62 80 `
  --container raw `
  --local-output data/communes.json
```

Param√®tres additionnels : `--departements ""` pour tout r√©cup√©rer, `--datalake-path` pour changer le chemin distant.

### 4.3 Exploration depuis le Data Lake

Lister/t√©l√©charger ce qui est dans ADLS :

```powershell
python analytics/data_loader.py list --csv-prefix csv/
python analytics/data_loader.py fetch --csv-prefix csv/ --json-prefix geo/ --save-local
```

Variables possibles :

- `--connection-string` ou variable `AZURE_STORAGE_CONNECTION_STRING`
- `--filesystem` (`raw`, `staging`, ‚Ä¶)
- `--keep-json` pour conserver les payloads brut.

## 5. Pr√©parer les tables analytiques

Deux options :

1. **Notebook interactif** : `analytics/notebooks/data_preparation.ipynb`
   - Auto-d√©tection du projet (`PROJECT_ROOT`).
   - Harmonise les colonnes (noms normalis√©s, zfill, conversions num√©riques).
   - Produit des tables `stg_*`, `dim_commune`, `bridge_commune_code_postal`.
   - Permet un export local Parquet (`data/prepared/silver/`) via `SAVE_TO_PARQUET = True`.

2. **Module r√©utilisable** : `analytics/lib/data_prep.py`
   - Fonction `prepare_tables()` renvoyant un dict `{nom_table: DataFrame}`.
   - Utilis√© par le script d‚Äôexport SQL (et testable en ligne de commande).

Exemple rapide :

```python
from analytics.lib.data_prep import prepare_tables, tables_summary

tables = prepare_tables()
print(tables_summary(tables))
```

## 6. Charger vers Azure SQL Database

### 6.1 Pr√©parer la connexion

Le script `analytics/export_to_sql.py` lit automatiquement :

- `Terraform/terraform.tfvars` (`sql_server_name`, `sql_admin_login`, `sql_admin_password`, `sql_database_name`)
- Variables d‚Äôenvironnement `AZURE_SQL_SERVER`, `AZURE_SQL_USERNAME`, `AZURE_SQL_PASSWORD`, `AZURE_SQL_DATABASE`
- Param√®tres CLI (`--server`, `--username`, ‚Ä¶)

Assurez-vous d‚Äôavoir le driver ODBC 18 (ou 17) install√©.

### 6.2 Commande d‚Äôexport

```powershell
python analytics/export_to_sql.py
```

Options utiles :

- `--chunksize 200` pour ajuster la taille des lots envoy√©s.
- `--schema analytics` pour changer le sch√©ma cible.
- `--preview` pour afficher uniquement le r√©sum√© des tables sans les charger.

Le script :

1. Pr√©pare les DataFrames via `prepare_tables()`.
2. Affiche un r√©sum√© des lignes/colonnes.
3. Tente de se connecter en testant plusieurs drivers (`ODBC 18`, `ODBC 17`, ‚Ä¶).
4. Ins√®re les donn√©es table par table (`replace` sur le premier lot, `append` ensuite).
5. Ignore les tables vides (ex : `dim_commune_geojson` si aucun contour n‚Äôest disponible).

### 6.3 V√©rification rapide

```sql
SELECT TABLE_NAME, COUNT(*) AS rows
FROM INFORMATION_SCHEMA.TABLES t
JOIN sys.tables s ON s.name = t.TABLE_NAME
WHERE t.TABLE_SCHEMA = 'dbo';
```
python generate_env.py  # gÈnËre .env ‡ partir de Terraform/terraform.tfvars si possible
$env:PYTHONPATH = "D:\data eng\Projet-Data-ENG"  # important pour pointer sur le package local
python -m uvicorn analytics.api.app.main:app --reload --port 8000
## 7. Exposer les tables via l‚ÄôAPI FastAPI
$env:PYTHONPATH = "D:\data eng\Projet-Data-ENG"  # important pour pointer sur le package local
"

# Projet-Data-ENG

Infrastructure-as-Code (Terraform) et scripts d ingestion pour provisionner un socle Azure Storage (landing + Data Lake), un Key Vault, un Data Factory et automatiser l import des communes via l API geo.api.gouv.fr.

## 1. Infrastructure

- Groupe de ressources Azure (nom et region parametrables)
- Compte de stockage Blob "landing" (HTTPS only, TLS1_2)
- Compte ADLS Gen2 (HNS actif) avec filesystems `raw`, `staging`, `curated`
- Azure Key Vault (Access Policies, purge protection)
- Azure Data Factory avec identite system-assigned
- Reseau virtuel avec sous-reseaux dedies Databricks (private/public + NSG)
- Workspace Azure Databricks en mode VNet injection
- Upload local optionnel des fichiers CSV via Terraform

Documentation detaillee : `docs/architecture.md`.

## 2. Prerequis

- Terraform 1.5+
- Azure CLI 2.52+
- Compte Azure avec droits de creation
- Python 3.10+ et `pip`

## 3. Deploiement rapide

```bash
cd Terraform
terraform init
terraform plan
terraform apply
```

Repondez `yes` pour appliquer. Les sorties utiles sont visibles via `terraform output`.

### Upload Terraform optionnel

1. Placer les CSV dans `uploads/landing/csv/`
2. Dans `Terraform/terraform.tfvars`, fixer `upload_files_enabled = true` (adapter `upload_source_dir`, `upload_container_name` si besoin)
3. `terraform apply`
4. Contrôler `terraform output upload_file_count`

## 4. Configuration projet

Parametres principaux (`Terraform/terraform.tfvars` non versionne) :
- `resource_group_name`
- `resource_group_location`
- `blob_storage_account_name`
- `datalake_storage_account_name`
- `key_vault_name`
- `data_factory_name`
- `kv_additional_reader_object_ids`
- Variables reseau Databricks (`vnet_name`, `vnet_address_space`, `databricks_*_subnet_*`)
- `databricks_workspace_name`
- Variables d upload optionnel (`upload_files_enabled`, `upload_source_dir`, `upload_container_name`)

Les comptes de stockage doivent respecter : minuscules, chiffres, 3-24 caracteres, nom globalement unique.

## 5. Ingestion des communes (API geo.api.gouv.fr)

Le repertoire `ingestion/` contient `fetch_communes.py`. Le script :
- interroge l API geo.api.gouv.fr (departements Hauts-de-France par defaut)
- enrichit la reponse (longitude/latitude, libelles departement/region, contour GeoJSON)
- construit un JSON unique et l envoie dans le conteneur Blob `landing`
- optionnellement, ecrit un fichier JSON local

### Installation dependances

```bash
python -m pip install -r requirements.txt
```

### Recuperer la chaine de connexion Azure Storage

```bash
cd Terraform
terraform output -raw blob_primary_connection_string
cd ..
```

### Execution standard

```bash
python ingestion/fetch_communes.py \
  --connection-string "DefaultEndpointsProtocol=..." \
  --departements 02 59 60 62 80 \
  --container landing \
  --local-output data/communes_hauts_de_france.json
```

Options utiles :
- Omettre `--departements` (ou passer `--departements ""`) pour recuperer toutes les communes
- `--blob-path` pour fixer le nom du JSON dans le conteneur
- `--api-key`, `--api-key-header` (X-API-Key par exemple) ou `--api-key-param` pour les APIs protegees

### Difficultes rencontrees

| Probleme | Cause | Solution |
|----------|-------|----------|
| `Connection string is either blank or malformed` | chaine non fournie | recuperer la chaine via Terraform/portail et la transmettre (`--connection-string` ou variable d environnement) |
| `AuthenticationFailed ... string to sign '/bselbrek/landing restype:container'` | `setx` pris en compte uniquement apres ouverture d un nouveau terminal | rouvrir la session ou passer la chaine en argument |
| `can't open file ... terraform\ingestion\fetch_communes.py` | commande lancee depuis le dossier Terraform | executer depuis la racine `D:\data eng\Projet-Data-ENG` |

### Commandes resumees

```bash
# Provision infra
cd Terraform
terraform init
terraform apply
cd ..

# Installation dependances Python
python -m pip install -r requirements.txt

# Recuperer la chaine de connexion
cd Terraform
terraform output -raw blob_primary_connection_string
cd ..

# Lancer l ingestion JSON
python ingestion/fetch_communes.py \
  --connection-string "DefaultEndpointsProtocol=..." \
  --departements 02 59 60 62 80 \
  --container landing \
  --local-output data/communes.json
```

## 6. Workspace Databricks (VNet injection)

Terraform cree :
- un reseau virtuel (`virtual_network_name`) et deux sous-reseaux dedies (`databricks_private_subnet_id`, `databricks_public_subnet_id`)
- deux Network Security Groups associes pour controler les flux (lances automatiques)
- un workspace Azure Databricks (`databricks_workspace_name`) deja rattache au VNet

Apres `terraform apply`, recupere les informations utiles :
```bash
cd Terraform
terraform output databricks_workspace_url
terraform output databricks_private_subnet_id
terraform output databricks_public_subnet_id
cd ..
```
Le workspace est pret a recevoir des clusters (mode VNet-injected). Les NSG permettent les communications intra-VNet et la sortie internet. Adapte les regles si ta politique de securite l exige.

## 7. Arborescence

```
Terraform/
  main.tf
  variables.tf
  providers.tf
  outputs.tf
  terraform.tfvars        # valeurs locales (ignorees)
docs/
  architecture.md
ingestion/
  fetch_communes.py
uploads/
  landing/
requirements.txt
.gitignore
README.md
```

Astuce : generer un `Terraform/terraform.tfvars.example` pour partager un gabarit sans secrets.

## 8. Nettoyage

```bash
cd Terraform
terraform destroy
```

## 9. Depannage rapide

- `az login` derriere proxy interceptant TLS : importer le certificat et fixer `REQUESTS_CA_BUNDLE`
- Noms comptes stockage rejetes : respecter le format (lowercase, 3-24, unique)
- Upload Terraform ignore : verifier `upload_files_enabled` et l extension `.csv`

## 10. Publier sur GitHub

```bash
git init
git add .
git commit -m "Initial infrastructure"
git remote add origin https://github.com/<organisation>/projet-data-eng.git
git push -u origin main
```

Ne pas versionner `uploads/` pour eviter toute fuite de donnees.

## 11. Ressources complementaires

- `docs/architecture.md`
- Terraform provider AzureRM : https://registry.terraform.io/providers/hashicorp/azurerm/latest
- Azure CLI : https://learn.microsoft.com/cli/azure
